"""
í•™ìŠµìš© í† í¬ë‚˜ì´ì € íŠ¸ë ˆì´ë‹ - ë¡œê·¸ ì¶”ê°€ ë²„ì „
"""
import os
import time
import argparse
import torch
from nanochat.tokenizer import RustBPETokenizer
from nanochat.common import get_base_dir
from nanochat.dataset import parquets_iter_batched

print("ğŸš€ tok_train.py ì‹œì‘!")
print("=" * 50)

# -----------------------------------------------------------------------------
# Parse command line arguments

parser = argparse.ArgumentParser(description='Train a BPE tokenizer')
parser.add_argument('--max_chars', type=int, default=1_000_000, help='Maximum characters (í•™ìŠµìš©ìœ¼ë¡œ ì‘ê²Œ)')
parser.add_argument('--doc_cap', type=int, default=10_000, help='Maximum characters per document')
parser.add_argument('--vocab_size', type=int, default=1000, help='Vocabulary size (í•™ìŠµìš©ìœ¼ë¡œ ì‘ê²Œ)')
args = parser.parse_args()

print(f"ğŸ“Š ì„¤ì •ê°’ë“¤:")
print(f"  - max_chars: {args.max_chars:,}")
print(f"  - doc_cap: {args.doc_cap:,}")
print(f"  - vocab_size: {args.vocab_size:,}")
print()

# -----------------------------------------------------------------------------
# Text iterator

def text_iterator():
    """
    1) Flatten the batches into a single iterator
    2) Cap each document to args.doc_cap characters
    3) Stop after args.max_chars characters total
    """
    print("ğŸ“ ë°ì´í„° ë¡œë”© ì¤‘...")
    chars_processed = 0
    doc_count = 0
    
    for batch in parquets_iter_batched():
        print(f"  ë°°ì¹˜ ì²˜ë¦¬ ì¤‘... (í˜„ì¬ê¹Œì§€ {chars_processed:,} ë¬¸ì ì²˜ë¦¬ë¨)")
        
        for row in batch:
            if chars_processed >= args.max_chars:
                print(f"âœ… ëª©í‘œ ë¬¸ì ìˆ˜ ë‹¬ì„±! ({args.max_chars:,} ë¬¸ì)")
                return
                
            text = row["text"]
            if args.doc_cap > 0:
                text = text[:args.doc_cap]
            
            chars_processed += len(text)
            doc_count += 1
            
            if doc_count % 100 == 0:
                print(f"    ğŸ“„ ë¬¸ì„œ {doc_count}ê°œ ì²˜ë¦¬ë¨")
            
            yield text

print("ğŸ”„ í…ìŠ¤íŠ¸ ì´í„°ë ˆì´í„° ìƒì„±...")
text_iter = text_iterator()

# -----------------------------------------------------------------------------
# Train the tokenizer
print("ğŸ§  í† í¬ë‚˜ì´ì € í›ˆë ¨ ì‹œì‘!")
print("=" * 30)

t0 = time.time()
print("  â†’ RustBPETokenizer.train_from_iterator í˜¸ì¶œ...")

tokenizer = RustBPETokenizer.train_from_iterator(text_iter, args.vocab_size)

t1 = time.time()
train_time = t1 - t0

print(f"âœ… í›ˆë ¨ ì™„ë£Œ!")
print(f"  â±ï¸  ì†Œìš” ì‹œê°„: {train_time:.2f}ì´ˆ")
print(f"  ğŸ“ ì–´íœ˜ í¬ê¸°: {args.vocab_size:,}ê°œ")

# -----------------------------------------------------------------------------
# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
print("\nğŸ§ª í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸:")
test_text = "Hello, world! This is a test."
encoded = tokenizer.encode(test_text)
decoded = tokenizer.decode(encoded)

print(f"  ì›ë³¸: {test_text}")
print(f"  í† í°í™”: {encoded}")
print(f"  ë³µì›: {decoded}")
print(f"  í† í° ìˆ˜: {len(encoded)}ê°œ")

# -----------------------------------------------------------------------------
# Save the tokenizer to disk
print("\nğŸ’¾ í† í¬ë‚˜ì´ì € ì €ì¥ ì¤‘...")
base_dir = get_base_dir()
tokenizer_dir = os.path.join(base_dir, "tokenizer")
tokenizer.save(tokenizer_dir)
print(f"âœ… ì €ì¥ ì™„ë£Œ: {tokenizer_dir}")

print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print("=" * 50)